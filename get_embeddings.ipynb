{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86688df6-9fe4-4a69-b782-f2bb8d35d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade setuptools wheel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d785a1c8-ca33-4e85-be04-870885db14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: biopython in c:\\users\\sanmams\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.79)\n",
      "Requirement already satisfied: numpy in c:\\users\\sanmams\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from biopython) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install biopython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcb5783-0ac7-41df-bb30-c6c1fe1b65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9e4cf7-c22c-481f-84c0-e3982681c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece h5py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20edbea-f132-423b-90e3-7837a28c8c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db632667-f68e-470a-81b9-ce96bf1ff7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee1724c-569c-4182-b428-5e4bf1bae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate datasets tqdm scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8834a834-7ccd-495d-86c0-fa3a8eee97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "os.chdir(\"./ProtT5_model\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import transformers, datasets\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n",
    "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')### only need one GPU for inference\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d69ab-9cdc-48d4-97ef-fccf8d417d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_path ='./DC.fasta'\n",
    "#using per_protein method to load proteins\n",
    "per_protein=True\n",
    "per_protein_path = \"./DC.h5\"\n",
    "per_residue=False\n",
    "sec_struct=False\n",
    "assert per_protein is True or per_residue is True or sec_struct is True, print(\n",
    "    \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f1489-6f1b-4f09-a32a-eb7d0e4f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"./ProtT5_model\")# here, you can download ProtT5 model to local file or load it via huggingface\n",
    "    model = model.to(device) \n",
    "    model = model.eval() \n",
    "    tokenizer = T5Tokenizer.from_pretrained('./ProtT5_model', do_lower_case=False)# load tokenlizer\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd69f81-56af-4e92-bad7-69955605116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###load fasta file\n",
    "def read_fasta( fasta_path, split_char=\"!\", id_field=0):\n",
    "    '''\n",
    "        Reads in fasta file containing multiple sequences.\n",
    "        Split_char and id_field allow to control identifier extraction from header.\n",
    "        E.g.: set split_char=\"|\" and id_field=1 for SwissProt/UniProt Headers.\n",
    "        Returns dictionary holding multiple sequences or only single\n",
    "        sequence, depending on input file.\n",
    "    '''\n",
    "\n",
    "    seqs = dict()\n",
    "    with open( fasta_path, 'r' ) as fasta_f:\n",
    "        for line in fasta_f:\n",
    "            # get uniprot ID from header and create new entry\n",
    "            if line.startswith('>'):\n",
    "                uniprot_id = line.replace('>', '').strip().split(split_char)[id_field]\n",
    "                # replace tokens that are mis-interpreted when loading h5\n",
    "                uniprot_id = uniprot_id.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "                seqs[ uniprot_id ] = ''\n",
    "            else:\n",
    "                # repl. all whie-space chars and join seqs spanning multiple lines, drop gaps and cast to upper-case\n",
    "                seq= ''.join( line.split() ).upper().replace(\"-\",\"\")\n",
    "                # repl. all non-standard AAs and map them to unknown/X\n",
    "                seq = seq.replace('U','X').replace('Z','X').replace('O','X')\n",
    "                seqs[ uniprot_id ] += seq\n",
    "    example_id=next(iter(seqs))\n",
    "    print(\"Read {} sequences.\".format(len(seqs)))\n",
    "    print(\"Example:\\n{}\\n{}\".format(example_id,seqs[example_id]))\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629eb20-0274-495b-9005-601cb7723130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "def get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct,\n",
    "                   max_residues=9000, max_seq_len=1000, max_batch=100 ):\n",
    "\n",
    "    if sec_struct:\n",
    "      sec_struct_model = load_sec_struct_model()\n",
    "\n",
    "    results = {\"residue_embs\" : dict(),\n",
    "               \"protein_embs\" : dict(),\n",
    "               \"sec_structs\" : dict()\n",
    "               }\n",
    "    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len\n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "                continue\n",
    "\n",
    "            if sec_struct: \n",
    "              d3_Yhat, d8_Yhat, diso_Yhat = sec_struct_model(embedding_repr.last_hidden_state)\n",
    "\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): \n",
    "                s_len = seq_lens[batch_idx]\n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
    "                if sec_struct: \n",
    "                    results[\"sec_structs\"][identifier] = torch.max( d3_Yhat[batch_idx,:s_len], dim=1 )[1].detach().cpu().numpy().squeeze()\n",
    "                if per_residue: \n",
    "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
    "                if per_protein: \n",
    "                    protein_emb = emb.mean(dim=0)\n",
    "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    passed_time=time.time()-start\n",
    "    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n",
    "    print('\\n############# EMBEDDING STATS #############')\n",
    "    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n",
    "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
    "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
    "        passed_time/60, avg_time ))\n",
    "    print('\\n############# END #############')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08facfd0-a241-468f-b7bd-1f23fbaddfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(emb_dict,out_path):\n",
    "    with h5py.File(str(out_path), \"w\") as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb1430-67e5-4a4b-bbde-4793e90d65e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f80e5b-0b3a-414b-b5cf-1deeece53c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_T5_model()\n",
    "\n",
    "seqs = read_fasta( seq_path )\n",
    "\n",
    "results = get_embeddings( model, tokenizer, seqs,\n",
    "                         per_residue, per_protein, sec_struct)\n",
    "\n",
    "if per_residue:\n",
    "  save_embeddings(results[\"residue_embs\"], per_residue_path)\n",
    "if per_protein:\n",
    "  save_embeddings(results[\"protein_embs\"], per_protein_path)\n",
    "if sec_struct:\n",
    "  write_prediction_fasta(results[\"sec_structs\"], sec_struct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b3c0b-5328-4a17-b3c1-c491403ed7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
